---
---

@string{aps = {American Physical Society,}}


@ARTICLE{11146415,
  author={Zhao, Yan and Cheng, Zhengxue and Li, Jiangchuan and Feng, Donghui and Gu, Qunshan and Wang, Qi and Lu, Guo and Song, Li},
  journal={IEEE Transactions on Image Processing}, 
  title={Instance-Adaptive Spatial-Temporal Enhancement for Efficient Video Compression}, 
  year={2025},
  volume={34},
  number={},
  pages={5776-5791},
  keywords={Videos;Adaptation models;Spatiotemporal phenomena;Overfitting;Decoding;Spatial resolution;Transformers;Video compression;Image coding;Correlation;Video compression;spatial-temporal enhancement;instance-adaptive overfitting;low-rank adaption},
  doi={10.1109/TIP.2025.3602648}
}

@ARTICLE{10870312,
  author={Zhang, Yuhong and Zhang, Hengsheng and Cheng, Zhengxue and Xie, Rong and Song, Li and Zhang, Wenjun},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={SSP-IR: Semantic and Structure Priors for Diffusion-Based Realistic Image Restoration}, 
  year={2025},
  volume={35},
  number={7},
  pages={6259-6272},
  keywords={Image restoration;Semantics;Degradation;Training;Diffusion models;Accuracy;Visualization;Noise reduction;Data mining;Superresolution;Image restoration;diffusion models;realistic super-resolution;MLLM},
  doi={10.1109/TCSVT.2025.3538772}
}

@inproceedings{10.1609/aaai.v39i12.33446,
  author = {Zhang, Junxuan and Cheng, Zhengxue and Zhao, Yan and Wang, Shihao and Zhou, Dajiang and Lu, Guo and Song, Li},
  title = {L3TC: leveraging RWKV for learned lossless low-complexity text compression},
  year = {2025},
  isbn = {978-1-57735-897-8},
  publisher = {AAAI Press},
  url = {https://doi.org/10.1609/aaai.v39i12.33446},
  doi = {10.1609/aaai.v39i12.33446},
  abstract = {Learning-based probabilistic models can be combined with an entropy coder for data compression. However, due to the high complexity of learning-based models, their practical application as text compressors has been largely overlooked. To address this issue, our work focuses on a low-complexity design while maintaining compression performance. We introduce a novel Learned Lossless Low-complexity Text Compression method (L3TC). Specifically, we conduct extensive experiments demonstrating that RWKV models achieve the fastest decoding speed with a moderate compression ratio, making it the most suitable backbone for our method. Second, we propose an outlier-aware tokenizer that uses a limited vocabulary to cover frequent tokens while allowing outliers to bypass the prediction and encoding. Third, we propose a novel high-rank reparameterization strategy that enhances the learning capability during training without increasing complexity during inference. Experimental results validate that our method achieves 48\% bit saving compared to gzip compressor. Besides, L3TC offers compression performance comparable to other learned compressors, with a 50\texttimes{} reduction in model parameters. More importantly, L3TC is the fastest among all learned compressors, providing real-time decoding speeds up to megabytes per second. Code â€” https://github.com/alipay/L3TC-leveraging-rwkv-for-learned-lossless-low-complexity-text-compression.git},
  booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
  articleno = {1473},
  numpages = {9},
  preview={l3tc2025.png},
  selected={true},
  series = {AAAI'25/IAAI'25/EAAI'25}
}


@misc{cheng2025omnivtlavisiontactilelanguageactionmodelsemanticaligned,
      title={OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing}, 
      author={Zhengxue Cheng and Yiqian Zhang and Wenkang Zhang and Haoyu Li and Keyu Wang and Li Song and Hengdi Zhang},
      year={2025},
      eprint={2508.08706},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      preview={vtla2025.png},
      selected={true},
      url={https://arxiv.org/abs/2508.08706} 
}


@inproceedings{Feng2025LALIC,
    author    = {Donghui Feng and Zhengxue Cheng and Shen Wang and Ronghua Wu and Hongwei Hu and Guo Lu and Li Song},
    title     = {Linear Attention Modeling for Learned Image Compression},
    booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2025},
    month={June},
    pages     = {1-10},
    url       = {https://arxiv.org/abs/2502.05741},
    selected={true},
    preview={cvpr2025.png}
}

@inproceedings{cheng2020cvpr,
    title={Learned Image Compression with Discretized Gaussian Mixture Likelihoods and Attention Modules},
    author={Cheng, Zhengxue and Sun, Heming and Takeuchi, Masaru and Katto, Jiro},
    journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    abstract={Image compression is a fundamental research field and many well-known compression standards have been developed for many decades. Recently, learned compression methods exhibit a fast development trend with promising results. However, there is still a performance gap between learned compression algorithms and reigning compression standards, especially in terms of widely used PSNR metric. In this paper, we explore the remaining redundancy of recent learned compression algorithms. We have found accurate entropy models for rate estimation largely affect the optimization of network parameters and thus affect the rate-distortion performance. Therefore, in this paper, we propose to use discretized Gaussian Mixture Likelihoods to parameterize the distributions of latent codes, which can achieve a more accurate and flexible entropy model. Besides, we take advantage of recent attention modules and incorporate them into network architecture to enhance the performance. Experimental results demonstrate our proposed method achieves a state-of-the-art performance compared to existing learned compression methods on both Kodak and high-resolution datasets. To our knowledge our approach is the first work to achieve comparable performance with latest compression standard Versatile Video Coding (VVC) regarding PSNR. More importantly, our approach generates more visually pleasant results when optimized by MS-SSIM. },
    year={2020},
    month={June},
    pdf={https://arxiv.org/abs/2001.01568},
    url={https://github.com/ZhengxueCheng/Learned-Image-Compression-with-GMM-and-Attention},
    selected={true},
    preview={cvpr2020.png}
}


@INPROCEEDINGS{cheng2019cvpr,
  author={Cheng, Zhengxue and Sun, Heming and Takeuchi, Masaru and Katto, Jiro},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Learning Image and Video Compression Through Spatial-Temporal Energy Compaction}, 
  year={2019},
  volume={},
  number={},
  pages={10063-10072},
  keywords={Measurement;Interpolation;Image coding;Transform coding;Video compression;Entropy;Compaction;Pattern recognition;Decoding;Standards;Image and Video Synthesis;Deep Learning ; Representation Learning; Video Analytics; Vision Applications and Systems},
  doi={10.1109/CVPR.2019.01031}}


@ARTICLE{cheng2020tmm,
  author={Cheng, Zhengxue and Sun, Heming and Takeuchi, Masaru and Katto, Jiro},
  journal={IEEE Transactions on Multimedia}, 
  title={Energy Compaction-Based Image Compression Using Convolutional AutoEncoder}, 
  abstract={Image compression has been an important research topic for many decades. Recently, deep learning has achieved great success in many computer vision tasks, and its use in image compression has gradually been increasing. In this paper, we present an energy compaction-based image compression architecture using a convolutional autoencoder (CAE) to achieve high coding efficiency. Our main contributions include three aspects: 1) we propose a CAE architecture for image compression by decomposing it into several down(up)sampling operations; 2) for our CAE architecture, we offer a mathematical analysis on the energy compaction property and we are the first work to propose a normalized coding gain metric in neural networks, which can act as a measurement of compression capability; 3) based on the coding gain metric, we propose an energy compaction-based bit allocation method, which adds a regularizer to the loss function during the training stage to help the CAE maximize the coding gain and achieve high compression efficiency. The experimental results demonstrate our proposed method outperforms BPG (HEVC-intra), in terms of the MS-SSIM quality metric. Additionally, we achieve better performance in comparison with existing bit allocation methods, and provide higher coding efficiency compared with state-of-the-art learning compression methods at high bit rates.},
  year={2020},
  volume={22},
  number={4},
  pages={860-873},
  keywords={Image coding;Bit rate;Convolutional codes;Quantization (signal);Image reconstruction;Compaction;Neural networks;Image compression;convolutional autoencoder;optimum bit allocation;energy compaction},
  doi={10.1109/TMM.2019.2938345}
}
