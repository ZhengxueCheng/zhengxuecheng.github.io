---
---

@STRING{ICLR = {International Conference on Learning Representations (ICLR)}}
@STRING{CVPR = {Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)}}
@STRING{ECCV = {Proc. of the European Conf. on Computer Vision (ECCV)}}
@STRING{ICCV = {Proc. of the IEEE International Conf. on Computer Vision (ICCV)}}
@STRING{ICRA = {Proc. of the IEEE International Conf. on Robotics and Automation (ICRA)}}
@STRING{THREEDV = {Proc. of the International Conf. on 3D Vision (3DV)}}
@STRING{RAL = {IEEE Robotics and Automation Letters (RA-L)}}
@STRING{TIP = {IEEE Trans. on  Image Processing (TIP)}}
@STRING{TPAMI = {IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI)}}
@STRING{TCSVT = {IEEE Trans. on Circuits and Systems for Video Technology (TCSVT)}}
@STRING{NEURIPS = {Advances in Neural Information Processing Systems (NeurIPS)}}
@STRING{ARXIV = {arXiv.org}}


@inproceedings{cheng2026tacobenchmarklosslesslossy,
    title={TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data}, 
    author={Zhengxue Cheng <sup>♯</sup> and Yan Zhao and Keyu Wang and Hengdi Zhang and Li Song},
    year={2026},
    booktitle = {ICLR},
    url={https://arxiv.org/abs/2602.09893},
    preview={TaCo2026.png},
    selected={true}
}

@ARTICLE{iaste2025,
    author={Zhao, Yan and Cheng <sup>♯</sup>, Zhengxue and Li, Jiangchuan and Feng, Donghui and Gu, Qunshan and Wang, Qi and Lu, Guo and Song, Li <sup>♯</sup>},
    journal={TIP}, 
    title={Instance-Adaptive Spatial-Temporal Enhancement for Efficient Video Compression}, 
    year={2025},
    volume={34},
    number={},
    pages={5776-5791},
    keywords={Videos;Adaptation models;Spatiotemporal phenomena;Overfitting;Decoding;Spatial resolution;Transformers;Video compression;Image coding;Correlation;Video compression;spatial-temporal enhancement;instance-adaptive overfitting;low-rank adaption},
    doi={10.1109/TIP.2025.3602648},
    preview={iaste2025.png},
    selected={true}
}


@inproceedings{Feng2025LALIC,
    author    = {Donghui Feng <sup>*</sup> and Zhengxue Cheng <sup>*</sup> and Shen Wang and Ronghua Wu and Hongwei Hu and Guo Lu and Li Song <sup>♯</sup>},
    title     = {Linear Attention Modeling for Learned Image Compression},
    booktitle = {CVPR},
    year      = {2025},
    month={June},
    pages     = {1-10},
    url       = {https://arxiv.org/abs/2502.05741},
    selected={true},
    preview={lalic2025.png}
}

@inproceedings{10.1145/3746027.3755136,
    author = {Wang, Ruiyan and Cheng <sup>♯</sup>, Zhengxue and Lin, Zonghao and Ling, Jun and Liu, Yuzhou and An, Yanru and Xie, Rong and Song <sup>♯</sup>, Li},
    title = {SemanticGarment: Semantic-Controlled Generation and Editing of 3D Gaussian Garments},
    year = {2025},
    isbn = {9798400720352},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3746027.3755136},
    doi = {10.1145/3746027.3755136},
    booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
    pages = {9793–9802},
    numpages = {10},
    keywords = {3d gaussian splatting, 3d multimodal generation, animation, clothing generation and editing},
    location = {Dublin, Ireland},
    series = {MM '25}
}

@inproceedings{10.1145/3746027.3758232,
    author = {Li, Bate and Zhong, Houqiang and Cheng <sup>♯</sup>, Zhengxue and Hu, Qiang and Wang, Qiang and Song <sup>♯</sup>, Li and Zhang <sup>♯</sup>, Wenjun},
    title = {MultiEgo: A Multi-View Egocentric Video Dataset for 4D Scene Reconstruction},
    year = {2025},
    isbn = {9798400720352},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3746027.3758232},
    doi = {10.1145/3746027.3758232},
    booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
    pages = {12882–12889},
    numpages = {8},
    keywords = {dynamic scene, egocentric video, free-viewpoint video},
    location = {Dublin, Ireland},
    series = {MM '25}
}

@inproceedings{10.1145/3746027.3758217,
    author = {Wang, Ruiyan and Zuo, Lin and Lin, Zonghao and Wang, Qiang and Cheng <sup>♯</sup>, Zhengxue and Xie, Rong and Ling <sup>♯</sup>, Jun and Song <sup>♯</sup>, Li},
    title = {PA-HOI: A Physics-Aware Human and Object Interaction Dataset},
    year = {2025},
    isbn = {9798400720352},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3746027.3758217},
    doi = {10.1145/3746027.3758217},
    booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia},
    pages = {12769–12775},
    numpages = {7},
    keywords = {human-object interaction, physical attribute-aware, text-driven hoi synthesis},
    location = {Dublin, Ireland},
    series = {MM '25}
}

@inproceedings{l3tc2025,
    author = {Zhang <sup>*</sup>, Junxuan and Cheng <sup>*♯</sup>, Zhengxue and Zhao, Yan and Wang, Shihao and Zhou, Dajiang and Lu, Guo and Song, Li},
    title = {L3TC: leveraging RWKV for learned lossless low-complexity text compression},
    year = {2025},
    isbn = {978-1-57735-897-8},
    publisher = {AAAI Press},
    url = {https://doi.org/10.1609/aaai.v39i12.33446},
    doi = {10.1609/aaai.v39i12.33446},
    booktitle = {Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
    articleno = {1473},
    numpages = {9},
    series = {AAAI'25/IAAI'25/EAAI'25},
    preview={l3tc2025.png},
    selected={true}
}


@ARTICLE{omniscalesr2025,
    author={Chai, Xinning and Cheng <sup>♯</sup>, Zhengxue and Zhang, Yuhong and Zhang, Hengsheng and Qin, Yingsheng and Yang, Yucai and Xie, Rong and Song <sup>♯</sup>, Li},
    journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
    title={OmniScaleSR: Unleashing Scale-Controlled Diffusion Prior for Faithful and Realistic Arbitrary-Scale Image Super-Resolution}, 
    year={2025},
    volume={},
    number={},
    pages={1-1},
    keywords={Superresolution;Diffusion models;Noise reduction;Adaptation models;Image reconstruction;Feature extraction;Transformers;Degradation;Videos;Text to image;Image super-resolution;diffusion models;scale-controlled generation;high-magnification reconstruction;arbitrary-scale super-resolution},
    doi={10.1109/TCSVT.2025.3642578},
    preview={omniscalesr.png},
    selected={true}
}


@ARTICLE{diffrestorer2025,
    author={Zhang, Yuhong and Zhang, Hengsheng and Chai, Xinning and Cheng <sup>♯</sup>, Zhengxue and Xie, Rong and Song <sup>♯</sup>, Li and Zhang, Wenjun},
    journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
    title={Diff-Restorer: Unleashing Visual Prompts for Diffusion-based Universal Image Restoration}, 
    year={2025},
    volume={},
    number={},
    pages={1-1},
    keywords={Image restoration;Degradation;Visualization;Diffusion models;Semantics;Adaptation models;Multitasking;Decoding;Image synthesis;Training;Image restoration;universal image restoration;diffusion models;visual prompt;CLIP},
    doi={10.1109/TCSVT.2025.3629686},
    preview={diffrestorer2025.png},
    selected={true}
}


@ARTICLE{sspir2025,
    author={Zhang, Yuhong and Zhang, Hengsheng and Cheng <sup>♯</sup>, Zhengxue and Xie, Rong and Song <sup>♯</sup>, Li and Zhang, Wenjun},
    journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
    title={SSP-IR: Semantic and Structure Priors for Diffusion-Based Realistic Image Restoration}, 
    year={2025},
    volume={35},
    number={7},
    pages={6259-6272},
    keywords={Image restoration;Semantics;Degradation;Training;Diffusion models;Accuracy;Visualization;Noise reduction;Data mining;Superresolution;Image restoration;diffusion models;realistic super-resolution;MLLM},
    doi={10.1109/TCSVT.2025.3538772},
    preview={sspir2025.png},
    selected={true}
}



@misc{cheng2025omnivtlavisiontactilelanguageactionmodelsemanticaligned,
    title={OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing}, 
    author={Zhengxue Cheng and Yiqian Zhang and Wenkang Zhang and Haoyu Li and Keyu Wang and Li Song and Hengdi Zhang},
    year={2025},
    eprint={2508.08706},
    archivePrefix={arXiv},
    primaryClass={cs.RO},
    preview={vtla2025.png},
    selected={true},
    url={https://arxiv.org/abs/2508.08706} 
}


@inproceedings{rateawarenerf,
    author = {Zhang <sup>*</sup>, Zhiyu and Lu <sup>*</sup>, Guo and Liang, Huanxiong and Cheng <sup>♯</sup>, Zhengxue and Tang, Anni and Song <sup>♯</sup>, Li},
    title = {Rate-aware Compression for NeRF-based Volumetric Video},
    year = {2024},
    isbn = {9798400706868},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3664647.3680970},
    doi = {10.1145/3664647.3680970},
    booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
    pages = {3974–3983},
    numpages = {10},
    keywords = {compression, nerf, rate estimation, volumetric video},
    location = {Melbourne VIC, Australia},
    series = {MM '24},
    selected={true},
    preview={nerf2024.png}
}


@inproceedings{cheng2020cvpr,
    title={Learned Image Compression with Discretized Gaussian Mixture Likelihoods and Attention Modules},
    author={Cheng, Zhengxue and Sun, Heming and Takeuchi, Masaru and Katto, Jiro},
    journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    abstract={Image compression is a fundamental research field and many well-known compression standards have been developed for many decades. Recently, learned compression methods exhibit a fast development trend with promising results. However, there is still a performance gap between learned compression algorithms and reigning compression standards, especially in terms of widely used PSNR metric. In this paper, we explore the remaining redundancy of recent learned compression algorithms. We have found accurate entropy models for rate estimation largely affect the optimization of network parameters and thus affect the rate-distortion performance. Therefore, in this paper, we propose to use discretized Gaussian Mixture Likelihoods to parameterize the distributions of latent codes, which can achieve a more accurate and flexible entropy model. Besides, we take advantage of recent attention modules and incorporate them into network architecture to enhance the performance. Experimental results demonstrate our proposed method achieves a state-of-the-art performance compared to existing learned compression methods on both Kodak and high-resolution datasets. To our knowledge our approach is the first work to achieve comparable performance with latest compression standard Versatile Video Coding (VVC) regarding PSNR. More importantly, our approach generates more visually pleasant results when optimized by MS-SSIM. },
    year={2020},
    month={June},
    pdf={https://arxiv.org/abs/2001.01568},
    url={https://github.com/ZhengxueCheng/Learned-Image-Compression-with-GMM-and-Attention},
    selected={true},
    preview={cvpr2020.png}
}

@ARTICLE{cheng2020tmm,
    author={Cheng, Zhengxue and Sun, Heming and Takeuchi, Masaru and Katto, Jiro},
    journal={IEEE Transactions on Multimedia}, 
    title={Energy Compaction-Based Image Compression Using Convolutional AutoEncoder}, 
    abstract={Image compression has been an important research topic for many decades. Recently, deep learning has achieved great success in many computer vision tasks, and its use in image compression has gradually been increasing. In this paper, we present an energy compaction-based image compression architecture using a convolutional autoencoder (CAE) to achieve high coding efficiency. Our main contributions include three aspects: 1) we propose a CAE architecture for image compression by decomposing it into several down(up)sampling operations; 2) for our CAE architecture, we offer a mathematical analysis on the energy compaction property and we are the first work to propose a normalized coding gain metric in neural networks, which can act as a measurement of compression capability; 3) based on the coding gain metric, we propose an energy compaction-based bit allocation method, which adds a regularizer to the loss function during the training stage to help the CAE maximize the coding gain and achieve high compression efficiency. The experimental results demonstrate our proposed method outperforms BPG (HEVC-intra), in terms of the MS-SSIM quality metric. Additionally, we achieve better performance in comparison with existing bit allocation methods, and provide higher coding efficiency compared with state-of-the-art learning compression methods at high bit rates.},
    year={2020},
    volume={22},
    number={4},
    pages={860-873},
    keywords={Image coding;Bit rate;Convolutional codes;Quantization (signal);Image reconstruction;Compaction;Neural networks;Image compression;convolutional autoencoder;optimum bit allocation;energy compaction},
    doi={10.1109/TMM.2019.2938345}
}


@INPROCEEDINGS{cheng2019cvpr,
    author={Cheng, Zhengxue and Sun, Heming and Takeuchi, Masaru and Katto, Jiro},
    booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
    title={Learning Image and Video Compression Through Spatial-Temporal Energy Compaction}, 
    year={2019},
    volume={},
    number={},
    pages={10063-10072},
    keywords={Measurement;Interpolation;Image coding;Transform coding;Video compression;Entropy;Compaction;Pattern recognition;Decoding;Standards;Image and Video Synthesis;Deep Learning ; Representation Learning; Video Analytics; Vision Applications and Systems},
    doi={10.1109/CVPR.2019.01031}
}