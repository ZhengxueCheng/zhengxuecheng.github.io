<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Zhengxue Cheng </title> <meta name="author" content="Zhengxue Cheng"> <meta name="description" content="Full publication list can be found on Google Scholar. &lt;sup&gt;*&lt;/sup&gt;equal contribution; &lt;sup&gt;♯&lt;/sup&gt;corresponding author."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9D%84%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhengxuecheng.github.io/publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zhengxue</span> Cheng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">Full publication list can be found on Google Scholar. <sup>*</sup>equal contribution; <sup>♯</sup>corresponding author.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2026</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/TaCo2026.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="TaCo2026.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cheng2026tacobenchmarklosslesslossy" class="col-sm-8"> <div class="title">TaCo: A Benchmark for Lossless and Lossy Codecs of Heterogeneous Tactile Data</div> <div class="author"> Zhengxue Cheng <sup>♯</sup>, Yan Zhao, Keyu Wang, Hengdi Zhang, and Li Song </div> <div class="periodical"> <em>In ICLR</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/iaste2025.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="iaste2025.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="iaste2025" class="col-sm-8"> <div class="title">Instance-Adaptive Spatial-Temporal Enhancement for Efficient Video Compression</div> <div class="author"> </div> <div class="periodical"> <em>TIP</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/TIP.2025.3602648" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/lalic2025.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="lalic2025.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Feng2025LALIC" class="col-sm-8"> <div class="title">Linear Attention Modeling for Learned Image Compression</div> <div class="author"> Donghui Feng <sup><sup>*</sup></sup>, Zhengxue Cheng <sup><sup>*</sup></sup>, Shen Wang, Ronghua Wu, Hongwei Hu, Guo Lu, and Li Song <sup>♯</sup> </div> <div class="periodical"> <em>In CVPR</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="10.1145/3746027.3755136" class="col-sm-8"> <div class="title">SemanticGarment: Semantic-Controlled Generation and Editing of 3D Gaussian Garments</div> <div class="author"> </div> <div class="periodical"> <em>In Proceedings of the 33rd ACM International Conference on Multimedia</em>, Dublin, Ireland, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1145/3746027.3755136" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="10.1145/3746027.3758232" class="col-sm-8"> <div class="title">MultiEgo: A Multi-View Egocentric Video Dataset for 4D Scene Reconstruction</div> <div class="author"> </div> <div class="periodical"> <em>In Proceedings of the 33rd ACM International Conference on Multimedia</em>, Dublin, Ireland, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1145/3746027.3758232" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="10.1145/3746027.3758217" class="col-sm-8"> <div class="title">PA-HOI: A Physics-Aware Human and Object Interaction Dataset</div> <div class="author"> </div> <div class="periodical"> <em>In Proceedings of the 33rd ACM International Conference on Multimedia</em>, Dublin, Ireland, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1145/3746027.3758217" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/l3tc2025.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="l3tc2025.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="l3tc2025" class="col-sm-8"> <div class="title">L3TC: leveraging RWKV for learned lossless low-complexity text compression</div> <div class="author"> </div> <div class="periodical"> <em>In Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1609/aaai.v39i12.33446" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/omniscalesr.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="omniscalesr.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="omniscalesr2025" class="col-sm-8"> <div class="title">OmniScaleSR: Unleashing Scale-Controlled Diffusion Prior for Faithful and Realistic Arbitrary-Scale Image Super-Resolution</div> <div class="author"> </div> <div class="periodical"> <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/TCSVT.2025.3642578" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/diffrestorer2025.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="diffrestorer2025.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="diffrestorer2025" class="col-sm-8"> <div class="title">Diff-Restorer: Unleashing Visual Prompts for Diffusion-based Universal Image Restoration</div> <div class="author"> </div> <div class="periodical"> <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/TCSVT.2025.3629686" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/sspir2025.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sspir2025.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sspir2025" class="col-sm-8"> <div class="title">SSP-IR: Semantic and Structure Priors for Diffusion-Based Realistic Image Restoration</div> <div class="author"> </div> <div class="periodical"> <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, Jun 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/TCSVT.2025.3538772" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/vtla2025.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vtla2025.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cheng2025omnivtlavisiontactilelanguageactionmodelsemanticaligned" class="col-sm-8"> <div class="title">OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing</div> <div class="author"> Zhengxue Cheng, Yiqian Zhang, Wenkang Zhang, Haoyu Li, Keyu Wang, Li Song, and Hengdi Zhang </div> <div class="periodical"> Jun 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/nerf2024.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="nerf2024.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rateawarenerf" class="col-sm-8"> <div class="title">Rate-aware Compression for NeRF-based Volumetric Video</div> <div class="author"> </div> <div class="periodical"> <em>In Proceedings of the 32nd ACM International Conference on Multimedia</em>, Melbourne VIC, Australia, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1145/3664647.3680970" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/cvpr2020.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cvpr2020.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cheng2020cvpr" class="col-sm-8"> <div class="title">Learned Image Compression with Discretized Gaussian Mixture Likelihoods and Attention Modules</div> <div class="author"> Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto </div> <div class="periodical"> <em>In </em>, Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2001.01568" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Image compression is a fundamental research field and many well-known compression standards have been developed for many decades. Recently, learned compression methods exhibit a fast development trend with promising results. However, there is still a performance gap between learned compression algorithms and reigning compression standards, especially in terms of widely used PSNR metric. In this paper, we explore the remaining redundancy of recent learned compression algorithms. We have found accurate entropy models for rate estimation largely affect the optimization of network parameters and thus affect the rate-distortion performance. Therefore, in this paper, we propose to use discretized Gaussian Mixture Likelihoods to parameterize the distributions of latent codes, which can achieve a more accurate and flexible entropy model. Besides, we take advantage of recent attention modules and incorporate them into network architecture to enhance the performance. Experimental results demonstrate our proposed method achieves a state-of-the-art performance compared to existing learned compression methods on both Kodak and high-resolution datasets. To our knowledge our approach is the first work to achieve comparable performance with latest compression standard Versatile Video Coding (VVC) regarding PSNR. More importantly, our approach generates more visually pleasant results when optimized by MS-SSIM. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="cheng2020tmm" class="col-sm-8"> <div class="title">Energy Compaction-Based Image Compression Using Convolutional AutoEncoder</div> <div class="author"> Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto </div> <div class="periodical"> <em>IEEE Transactions on Multimedia</em>, Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TMM.2019.2938345" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Image compression has been an important research topic for many decades. Recently, deep learning has achieved great success in many computer vision tasks, and its use in image compression has gradually been increasing. In this paper, we present an energy compaction-based image compression architecture using a convolutional autoencoder (CAE) to achieve high coding efficiency. Our main contributions include three aspects: 1) we propose a CAE architecture for image compression by decomposing it into several down(up)sampling operations; 2) for our CAE architecture, we offer a mathematical analysis on the energy compaction property and we are the first work to propose a normalized coding gain metric in neural networks, which can act as a measurement of compression capability; 3) based on the coding gain metric, we propose an energy compaction-based bit allocation method, which adds a regularizer to the loss function during the training stage to help the CAE maximize the coding gain and achieve high compression efficiency. The experimental results demonstrate our proposed method outperforms BPG (HEVC-intra), in terms of the MS-SSIM quality metric. Additionally, we achieve better performance in comparison with existing bit allocation methods, and provide higher coding efficiency compared with state-of-the-art learning compression methods at high bit rates.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="cheng2019cvpr" class="col-sm-8"> <div class="title">Learning Image and Video Compression Through Spatial-Temporal Energy Compaction</div> <div class="author"> Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto </div> <div class="periodical"> <em>In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/CVPR.2019.01031" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Zhengxue Cheng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>